{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9cKNgSDdJh"
      },
      "source": [
        "# High-performance image generation using Stable Diffusion in KerasCV\n",
        "\n",
        "**Authors:** [fchollet](https://twitter.com/fchollet), [lukewood](https://twitter.com/luke_wood_ml), [divamgupta](https://github.com/divamgupta)<br>\n",
        "**Date created:** 2022/09/25<br>\n",
        "**Last modified:** 2022/09/25<br>\n",
        "**Description:** Generate new images using KerasCV's StableDiffusion model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjz_RpALDdJk"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this guide, we will show how to generate novel images based on a text prompt using\n",
        "the KerasCV implementation of [stability.ai](https://stability.ai/)'s text-to-image model,\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion).\n",
        "\n",
        "Stable Diffusion is a powerful, open-source text-to-image generation model.  While there\n",
        "exist multiple open-source implementations that allow you to easily create images from\n",
        "textual prompts, KerasCV's offers a few distinct advantages.\n",
        "These include [XLA compilation](https://www.tensorflow.org/xla) and\n",
        "[mixed precision](https://www.tensorflow.org/guide/mixed_precision) support,\n",
        "which together achieve state-of-the-art generation speed.\n",
        "\n",
        "In this guide, we will explore KerasCV's Stable Diffusion implementation, show how to use\n",
        "these powerful performance boosts, and explore the performance benefits\n",
        "that they offer.\n",
        "\n",
        "To get started, let's install a few dependencies and sort out some imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "McHUiY7_DdJl"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow keras_cv --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhVVHQYXDdJl",
        "outputId": "d037f3ea-68f1-4450-d34c-560f12fc64be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import keras_cv\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BgCv2NVDdJm"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Unlike most tutorials, where we first explain a topic then show how to implement it,\n",
        "with text-to-image generation it is easier to show instead of tell.\n",
        "\n",
        "Check out the power of `keras_cv.models.StableDiffusion()`.\n",
        "\n",
        "First, we construct a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU6FZPKDDdJm",
        "outputId": "7178a77e-369a-4df2-8f40-523af1e0a36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
          ]
        }
      ],
      "source": [
        "model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIy1paB7DdJn"
      },
      "source": [
        "Next, we give it a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gH6NfBhDdJn"
      },
      "outputs": [],
      "source": [
        "images = model.text_to_image(\"f\", batch_size=3)\n",
        "\n",
        "\n",
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "\n",
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSraYJVyDdJo"
      },
      "source": [
        "Pretty incredible!\n",
        "\n",
        "But that's not all this model can do.  Let's try a more complex prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OaFiqTPpDdJo",
        "outputId": "edd2f71a-18a0-4c93-c2f8-5b7500b97b05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8ace7fd19a59>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m images = model.text_to_image(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"photograph of my mom, my dad and my elder brother and sister with me, a baby. living our best life as a middle class family in saudi arabia\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"golden color, high quality, highly detailed, elegant, sharp focus, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"concept art, character concepts, digital painting, mystery, adventure\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/stable_diffusion.py\u001b[0m in \u001b[0;36mtext_to_image\u001b[0;34m(self, prompt, negative_prompt, batch_size, num_steps, unconditional_guidance_scale, seed)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mencoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         return self.generate_image(\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/stable_diffusion.py\u001b[0m in \u001b[0;36mgenerate_image\u001b[0;34m(self, encoded_text, negative_prompt, batch_size, num_steps, unconditional_guidance_scale, diffusion_noise, seed)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlatent_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent\u001b[0m  \u001b[0;31m# Set aside the previous latent vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_timestep_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             unconditional_latent = self.diffusion_model.predict_on_batch(\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munconditional_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2601\u001b[0m             )\n\u001b[1;32m   2602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2603\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'diffusion_model/spatial_transformer/basic_transformer_block/cross_attention/Softmax' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n      yield self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-9-8ace7fd19a59>\", line 1, in <cell line: 1>\n      images = model.text_to_image(\n    File \"/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/stable_diffusion.py\", line 78, in text_to_image\n      return self.generate_image(\n    File \"/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/stable_diffusion.py\", line 212, in generate_image\n      unconditional_latent = self.diffusion_model.predict_on_batch(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2603, in predict_on_batch\n      outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/diffusion_model.py\", line 260, in call\n      x = self.transformer_block([x, context])\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/diffusion_model.py\", line 278, in call\n      x = self.attn1([self.norm1(inputs), None]) + inputs\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras_cv/models/stable_diffusion/diffusion_model.py\", line 307, in call\n      weights = keras.activations.softmax(score)  # (bs, num_heads, time, time)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/activations.py\", line 85, in softmax\n      output = tf.nn.softmax(x, axis=axis)\nNode: 'diffusion_model/spatial_transformer/basic_transformer_block/cross_attention/Softmax'\nOOM when allocating tensor with shape[6,8,4096,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node diffusion_model/spatial_transformer/basic_transformer_block/cross_attention/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_62849]"
          ]
        }
      ],
      "source": [
        "images = model.text_to_image(\n",
        "    \"i\"\n",
        "    \"golden color, high quality, highly detailed, elegant, sharp focus, \"\n",
        "    \"concept art, character concepts, digital painting, mystery, adventure\",\n",
        "    batch_size=6,\n",
        ")\n",
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5gTJqMRDdJo"
      },
      "source": [
        "The possibilities are literally endless (or at least extend to the boundaries of\n",
        "Stable Diffusion's latent manifold)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuDCZOl1DdJp"
      },
      "source": [
        "## Wait, how does this even work?\n",
        "\n",
        "Unlike what you might expect at this point, StableDiffusion doesn't actually run on magic.\n",
        "It's a kind of \"latent diffusion model\". Let's dig into what that means.\n",
        "\n",
        "You may be familiar with the idea of _super-resolution_:\n",
        "it's possible to train a deep learning model to _denoise_ an input image -- and thereby turn it into a higher-resolution\n",
        "version. The deep learning model doesn't do this by magically recovering the information that's missing from the noisy, low-resolution\n",
        "input -- rather, the model uses its training data distribution to hallucinate the visual details that would be most likely\n",
        "given the input. To learn more about super-resolution, you can check out the following Keras.io tutorials:\n",
        "\n",
        "- [Image Super-Resolution using an Efficient Sub-Pixel CNN](https://keras.io/examples/vision/super_resolution_sub_pixel/)\n",
        "- [Enhanced Deep Residual Networks for single-image super-resolution](https://keras.io/examples/vision/edsr/)\n",
        "\n",
        "![Super-resolution](https://i.imgur.com/M0XdqOo.png)\n",
        "\n",
        "When you push this idea to the limit, you may start asking -- what if we just run such a model on pure noise?\n",
        "The model would then \"denoise the noise\" and start hallucinating a brand new image. By repeating the process multiple\n",
        "times, you can get turn a small patch of noise into an increasingly clear and high-resolution artificial picture.\n",
        "\n",
        "This is the key idea of latent diffusion, proposed in\n",
        "[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) in 2020.\n",
        "To understand diffusion in depth, you can check the Keras.io tutorial\n",
        "[Denoising Diffusion Implicit Models](https://keras.io/examples/generative/ddim/).\n",
        "\n",
        "![Denoising diffusion](https://i.imgur.com/FSCKtZq.gif)\n",
        "\n",
        "Now, to go from latent diffusion to a text-to-image system,\n",
        "you still need to add one key feature: the ability to control the generated visual contents via prompt keywords.\n",
        "This is done via \"conditioning\", a classic deep learning technique which consists of concatenating to the\n",
        "noise patch a vector that represents a bit of text, then training the model on a dataset of {image: caption} pairs.\n",
        "\n",
        "This gives rise to the Stable Diffusion architecture. Stable Diffusion consists of three parts:\n",
        "\n",
        "- A text encoder, which turns your prompt into a latent vector.\n",
        "- A diffusion model, which repeatedly \"denoises\" a 64x64 latent image patch.\n",
        "- A decoder, which turns the final 64x64 latent patch into a higher-resolution 512x512 image.\n",
        "\n",
        "First, your text prompt gets projected into a latent vector space by the text encoder,\n",
        "which is simply a pretrained, frozen language model. Then that prompt vector is concatenated\n",
        "to a randomly generated noise patch, which is repeatedly \"denoised\" by the diffusion model over a series\n",
        "of \"steps\" (the more steps you run the clearer and nicer your image will be -- the default value is 50 steps).\n",
        "\n",
        "Finally, the 64x64 latent image is sent through the decoder to properly render it in high resolution.\n",
        "\n",
        "![The Stable Diffusion architecture](https://i.imgur.com/2uC8rYJ.png)\n",
        "\n",
        "All-in-all, it's a pretty simple system -- the Keras implementation\n",
        "fits in four files that represent less than 500 lines of code in total:\n",
        "\n",
        "- [text_encoder.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/text_encoder.py): 87 LOC\n",
        "- [diffusion_model.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/diffusion_model.py): 181 LOC\n",
        "- [decoder.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/decoder.py): 86 LOC\n",
        "- [stable_diffusion.py](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py): 106 LOC\n",
        "\n",
        "But this relatively simple system starts looking like magic once you train on billions of pictures and their captions.\n",
        "As Feynman said about the universe: _\"It's not complicated, it's just a lot of it!\"_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWpqJnZrDdJp"
      },
      "source": [
        "## Perks of KerasCV\n",
        "\n",
        "With several implementations of Stable Diffusion publicly available why should you use\n",
        "`keras_cv.models.StableDiffusion`?\n",
        "\n",
        "Aside from the easy-to-use API, KerasCV's Stable Diffusion model comes\n",
        "with some powerful advantages, including:\n",
        "\n",
        "- Graph mode execution\n",
        "- XLA compilation through `jit_compile=True`\n",
        "- Support for mixed precision computation\n",
        "\n",
        "When these are combined, the KerasCV Stable Diffusion model runs orders of magnitude\n",
        "faster than naive implementations.  This section shows how to enable all of these\n",
        "features, and the resulting performance gain yielded from using them.\n",
        "\n",
        "For the purposes of comparison, we ran benchmarks comparing the runtime of the\n",
        "[HuggingFace diffusers](https://github.com/huggingface/diffusers) implementation of\n",
        "Stable Diffusion against the KerasCV implementation.\n",
        "Both implementations were tasked to generate 3 images with a step count of 50 for each\n",
        "image.  In this benchmark, we used a Tesla T4 GPU.\n",
        "\n",
        "[All of our benchmarks are open source on GitHub, and may be re-run on Colab to\n",
        "reproduce the results.](https://github.com/LukeWood/stable-diffusion-performance-benchmarks)\n",
        "The results from the benchmark are displayed in the table below:\n",
        "\n",
        "\n",
        "| GPU        | Model                  | Runtime   |\n",
        "|------------|------------------------|-----------|\n",
        "| Tesla T4   | KerasCV (Warm Start)   | **28.97s**|\n",
        "| Tesla T4   | diffusers (Warm Start) | 41.33s    |\n",
        "| Tesla V100 | KerasCV (Warm Start)   | **12.45** |\n",
        "| Tesla V100 | diffusers (Warm Start) | 12.72     |\n",
        "\n",
        "\n",
        "30% improvement in execution time on the Tesla T4!.  While the improvement is much lower\n",
        "on the V100, we generally expect the results of the benchmark to consistently favor the KerasCV\n",
        "across all NVIDIA GPUs.\n",
        "\n",
        "For the sake of completeness, both cold-start and warm-start generation times are\n",
        "reported. Cold-start execution time includes the one-time cost of model creation and compilation,\n",
        "and is therefore negligible in a production environment (where you would reuse the same model instance\n",
        "many times). Regardless, here are the cold-start numbers:\n",
        "\n",
        "\n",
        "| GPU        | Model                  | Runtime |\n",
        "|------------|------------------------|---------|\n",
        "| Tesla T4   | KerasCV (Cold Start)   | 83.47s  |\n",
        "| Tesla T4   | diffusers (Cold Start) | 46.27s  |\n",
        "| Tesla V100 | KerasCV (Cold Start)   | 76.43   |\n",
        "| Tesla V100 | diffusers (Cold Start) | 13.90   |\n",
        "\n",
        "\n",
        "While the runtime results from running this guide may vary, in our testing the KerasCV\n",
        "implementation of Stable Diffusion is significantly faster than its PyTorch counterpart.\n",
        "This may be largely attributed to XLA compilation.\n",
        "\n",
        "**Note: The performance benefits of each optimization can vary\n",
        "significantly between hardware setups.**\n",
        "\n",
        "To get started, let's first benchmark our unoptimized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFrKF14RDdJq"
      },
      "outputs": [],
      "source": [
        "benchmark_result = []\n",
        "start = time.time()\n",
        "images = model.text_to_image(\n",
        "    \"A cute otter in a rainbow whirlpool holding shells, watercolor\",\n",
        "    batch_size=3,\n",
        ")\n",
        "end = time.time()\n",
        "benchmark_result.append([\"Standard\", end - start])\n",
        "plot_images(images)\n",
        "\n",
        "print(f\"Standard model: {(end - start):.2f} seconds\")\n",
        "keras.backend.clear_session()  # Clear session to preserve memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMSsY3DGDdJq"
      },
      "source": [
        "### Mixed precision\n",
        "\n",
        "\"Mixed precision\" consists of performing computation using `float16`\n",
        "precision, while storing weights in the `float32` format.\n",
        "This is done to take advantage of the fact that `float16` operations are backed by\n",
        "significantly faster kernels than their `float32` counterparts on modern NVIDIA GPUs.\n",
        "\n",
        "Enabling mixed precision computation in Keras\n",
        "(and therefore for `keras_cv.models.StableDiffusion`) is as simple as calling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZubLHH7DdJq"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlg3NLCJDdJq"
      },
      "source": [
        "That's all.  Out of the box - it just works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbgTAwh6DdJr"
      },
      "outputs": [],
      "source": [
        "model = keras_cv.models.StableDiffusion()\n",
        "\n",
        "print(\"Compute dtype:\", model.diffusion_model.compute_dtype)\n",
        "print(\n",
        "    \"Variable dtype:\",\n",
        "    model.diffusion_model.variable_dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3JaunjRDdJr"
      },
      "source": [
        "As you can see, the model constructed above now uses mixed precision computation;\n",
        "leveraging the speed of `float16` operations for computation, while storing variables\n",
        "in `float32` precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIIFyueDDdJr"
      },
      "outputs": [],
      "source": [
        "# Warm up model to run graph tracing before benchmarking.\n",
        "model.text_to_image(\"warming up the model\", batch_size=3)\n",
        "\n",
        "start = time.time()\n",
        "images = model.text_to_image(\n",
        "    \"a cute magical flying dog, fantasy art, \"\n",
        "    \"golden color, high quality, highly detailed, elegant, sharp focus, \"\n",
        "    \"concept art, character concepts, digital painting, mystery, adventure\",\n",
        "    batch_size=3,\n",
        ")\n",
        "end = time.time()\n",
        "benchmark_result.append([\"Mixed Precision\", end - start])\n",
        "plot_images(images)\n",
        "\n",
        "print(f\"Mixed precision model: {(end - start):.2f} seconds\")\n",
        "keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXItp0cDdJr"
      },
      "source": [
        "### XLA Compilation\n",
        "\n",
        "TensorFlow comes with the\n",
        "[XLA: Accelerated Linear Algebra](https://www.tensorflow.org/xla) compiler built-in.\n",
        "`keras_cv.models.StableDiffusion` supports a `jit_compile` argument out of the box.\n",
        "Setting this argument to `True` enables XLA compilation, resulting in a significant\n",
        "speed-up.\n",
        "\n",
        "Let's use this below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8HTFBNgDdJr"
      },
      "outputs": [],
      "source": [
        "# Set back to the default for benchmarking purposes.\n",
        "keras.mixed_precision.set_global_policy(\"float32\")\n",
        "\n",
        "model = keras_cv.models.StableDiffusion(jit_compile=True)\n",
        "# Before we benchmark the model, we run inference once to make sure the TensorFlow\n",
        "# graph has already been traced.\n",
        "images = model.text_to_image(\"An avocado armchair\", batch_size=3)\n",
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj3ZZJjJDdJs"
      },
      "source": [
        "Let's benchmark our XLA model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjdup4JHDdJs"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "images = model.text_to_image(\n",
        "    \"A cute otter in a rainbow whirlpool holding shells, watercolor\",\n",
        "    batch_size=3,\n",
        ")\n",
        "end = time.time()\n",
        "benchmark_result.append([\"XLA\", end - start])\n",
        "plot_images(images)\n",
        "\n",
        "print(f\"With XLA: {(end - start):.2f} seconds\")\n",
        "keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSdPF-ydDdJs"
      },
      "source": [
        "On an A100 GPU, we get about a 2x speedup.  Fantastic!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxeXmUAnDdJs"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "So, how do you assemble the world's most performant stable diffusion inference\n",
        "pipeline (as of September 2022).\n",
        "\n",
        "With these two lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PX3krO2DdJs"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "model = keras_cv.models.StableDiffusion(jit_compile=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1XdXepsDdJt"
      },
      "source": [
        "And to use it..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYxOglN_DdJt"
      },
      "outputs": [],
      "source": [
        "# Let's make sure to warm up the model\n",
        "images = model.text_to_image(\n",
        "    \"Teddy bears conducting machine learning research\",\n",
        "    batch_size=3,\n",
        ")\n",
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qPQNulFDdJt"
      },
      "source": [
        "Exactly how fast is it?\n",
        "Let's find out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omutoPuwDdJt"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "images = model.text_to_image(\n",
        "    \"A mysterious dark stranger visits the great pyramids of egypt, \"\n",
        "    \"high quality, highly detailed, elegant, sharp focus, \"\n",
        "    \"concept art, character concepts, digital painting\",\n",
        "    batch_size=3,\n",
        ")\n",
        "end = time.time()\n",
        "benchmark_result.append([\"XLA + Mixed Precision\", end - start])\n",
        "plot_images(images)\n",
        "\n",
        "print(f\"XLA + mixed precision: {(end - start):.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNlgOZ6xDdJt"
      },
      "source": [
        "Let's check out the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPurbLVjDdJt"
      },
      "outputs": [],
      "source": [
        "print(\"{:<22} {:<22}\".format(\"Model\", \"Runtime\"))\n",
        "for result in benchmark_result:\n",
        "    name, runtime = result\n",
        "    print(\"{:<22} {:<22}\".format(name, runtime))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnP0GzDeDdJt"
      },
      "source": [
        "It only took our fully-optimized model four seconds to generate three novel images from\n",
        "a text prompt on an A100 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MzzQsCNDdJu"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "KerasCV offers a state-of-the-art implementation of Stable Diffusion -- and\n",
        "through the use of XLA and mixed precision, it delivers the fastest Stable Diffusion pipeline available as of September 2022.\n",
        "\n",
        "Normally, at the end of a keras.io tutorial we leave you with some future directions to continue in to learn.\n",
        "This time, we leave you with one idea:\n",
        "\n",
        "**Go run your own prompts through the model! It is an absolute blast!**\n",
        "\n",
        "If you have your own NVIDIA GPU, or a M1 MacBookPro, you can also run the model locally on your machine.\n",
        "(Note that when running on a M1 MacBookPro, you should not enable mixed precision, as it is not yet well supported\n",
        "by Apple's Metal runtime.)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "generate_images_with_stable_diffusion",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}